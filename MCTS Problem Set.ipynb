{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search Mini Problem Set\n",
    "\n",
    "Welcome to Starfleet Academy! You are headed to your first class in Game Tactics, a necessary prerequisite if you want to qualify for the officer training track. The teacher of Game Tactics is Stonn, a notoriously xenophobic Vulcan who has never forgotten his humiliation at the hands of a human and a halfbreed. His prejudice is the only obstacle between you and the yellow uniform, but as long as you keep your head down it should only be a minor ordeal. You enter the classroom, seeing students putting away their books and a clock that reads 10:00. Daylight savings time has struck again!\n",
    "\n",
    "\"Cadet, since you finally deigned to grace us with your presence, could you show me your no doubt unparalleled primate intelligence by playing me in a simple game of Connect Four?\"\n",
    "\n",
    "There's no way you can beat him with your current skills. To win this challenge (complete this problem set), you must learn and employ the uniquely human tree search of MCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what is a tree search?\n",
    "Trees are a special case of the graph problems previously seen in class. For example, consider Tic Tac Toe. From the starting blank board, each choice of where to draw an X is a possible future, followed by each choice of where to draw an O. A planner can look at this sprawl of futures and choose which action will most likely lead to victory.\n",
    "\n",
    "![Example 1](./example_1.png)\n",
    "\n",
    "## Breadth First Tree Search\n",
    "\n",
    "In a breadth first tree search the planner considers potential boards in order of depth. All boards that are one turn in the future are considered first, followed by the boards two turns away, until every potential board has been considered. The planner then chooses the best move to make based on whether the move will lead to victory or to defeat. In the following example, a breadth first search would identify that all other moves lead to a loss and instead pick the rightmost move.\n",
    "\n",
    "![Example 2](./example_2.png)\n",
    "\n",
    "## Monte Carlo Tree Search\n",
    "\n",
    "The problem with breadth first search is that it isn't at all clever. Tic Tac Toe is one of the simpler games in existence, but there are nearly three-hundred sixty thousand possible sets of moves for a BFS-based planner to consider. In a game with less constrained movement, like chess, this number exceeds the number of atoms in the known universe after looking only a couple of turns into the future. A Breadth First Search is too tied up with being logical and provably correct. Monte Carlo Tree Search leaps ahead to impulsively go where no search has gone before. In simpler terms, BFS is Spock while MCTS is Kirk.\n",
    "\n",
    "MCTS performs its search by repeatedly imagining play-throughs of the game or scenario, traveling down the entire branch of the game tree until it terminates. Based on how this play-through went, MCTS then updates the value of each node (move) involved based on whether it won or lost the playthrough. The Monte Carlo component comes from the fact that it chooses moves at random, not based on heuristics or visit count. This nondeterminism greatly increases the potential space it can explore even though its exploration will be much less rigorous.\n",
    "\n",
    "For example, let's look at the BFS tree above. The bad red moves have a high probability of loss because in 1/5 of the playthroughs X will instantly win. The correct, rightmost move will have a lower probability of a loss because X doesn't have this 1/5 chance of winning. MCTS will discard the red moves because of this higher loss probability, assigning them lower values whenever it selects them during a randomized play-through.\n",
    "\n",
    "## MCTS Algorithm\n",
    "\n",
    "[Check out the paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6145622&tag=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you write your very own MCTS bot, you need to get sped up on the API you will be using. But even before getting into the API, please run the following to import the API and some boilerplate code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import algo\n",
    "import sim\n",
    "from tests import *\n",
    "\n",
    "from game import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Board` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Board` class is a template class that represents the state of a board at a single point in a game. We've created a `ConnectFourBoard` subclass that handles the mechanics for you. For any `Board` instance `board`, you have access to the following methods:\n",
    "\n",
    "**`board.get_legal_actions()`:** Returns a python set of `Action` class instances. Each element in the set is a valid action that can be applied to the `board` to create a new `Board` instance. See the `Action` API section below.\n",
    "\n",
    "**`board.is_terminal()`:** Returns `True` if `board` is a endgame board. Returns `False` otherwise.\n",
    "\n",
    "**`board.current_player_id()`:** Returns an integer that represents which player is expected to play next. For example, if this method returns 0, then the player who is the first player in some simulation of a game should be the next one to play an action. This is used internally in the `Simulation` class for bookkeeping, but you will need it when you do the backup step of the MCTS algorithm.\n",
    "\n",
    "**`board.reward_vector()`:** Returns a $n$-element tuple, where $n$ is the number of players, that contains the rewards earned by each player at this particular `board`. For Connect Four, $n=2$. Thus this method may return something like `(1,-1)`, meaning the player with ID 0 had a reward of 1, and the player with ID 1 has reward -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Action` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Action` class is for representing a single action that is meant to alter a `board`. We have written a `ConnectFourAction` subclass for you. For any `Action` instance `action`, you will only need the following method:\n",
    "\n",
    "**`action.apply(board)`:** Given a `Board` instance `board`, returns a new `Board` instance that represents the board after that action has been performed. If the `action` cannot be applied, an error is thrown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Player` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Node` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Simulation` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Policy\n",
    "The first step is to implement the default policy, which plays through an entire game session. It chooses actions at random, applying them to the board until the game is over. It then returns the reward vector of the finished board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# randomly picking moves to reach the end game\n",
    "# Input: a board that want to start randomly picking moves\n",
    "#        the color of computer (black or red)\n",
    "# Output: the reward vector when the game terminates\n",
    "###########################################################\n",
    "def default_policy(board):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_default_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Policy\n",
    "The tree policy performs a depth-first search of the tree using `best_child` and `expand`. If it encounters an unexpanded node it will return the expanded child of that node. Otherwise, it continues its search in the best child of the current node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# get the best child from this node (using heuristic)\n",
    "# Input: a node, which we want to find the best child of\n",
    "#        c, the exploitation constant\n",
    "# Output: the child node\n",
    "###########################################################\n",
    "def best_child(node, c):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_best_child()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand\n",
    "This expands a node that has unexpanded children. It must get all current children of the node and all possible children of the node then add one of the possible children to the node. It should then return this newly added child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# expand a node since it is not fully expanded\n",
    "# Input: a node that want to be expanded\n",
    "# Output: the child node\n",
    "###########################################################\n",
    "def expand(node):\n",
    "    # Implement this\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_expand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Policy\n",
    "Now that you have `best_child` and `expand` it's time to policy some trees! As above, the tree policy performs a depth-first search of the tree using best_child and expand. If it encounters an unexpanded node it will return the expanded child of that node. Otherwise, it continues its search in the best child of the current node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# heuristically search to the leaf level\n",
    "# Input: a node that want to search down and the\n",
    "#        exploitation value c\n",
    "# Output: the leaf node that we expand till\n",
    "##########################################################\n",
    "def tree_policy(node, c):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tree_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup\n",
    "Now its time to make a way to turn the reward from `default_policy` into the information that `tree_policy` needs. `backup` should take the terminal state and reward from `default_policy` and proceed up the tree, updating the nodes on its path based on the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# reward update for the tree after one simulation\n",
    "# Input: a node that we want to backup from\n",
    "#        reward value\n",
    "# Output: nothing\n",
    "###########################################################\n",
    "def backup(node, reward_vector):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search (UCT)\n",
    "Time to put everything together! Keep running `tree_policy`, `default_policy`, and `backup` until you run out of time! Finally, return the best child's associated action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# monte carlo tree search algorithm using UCT heuristic\n",
    "# Input: class Board represents the current game board\n",
    "#        time limit of calculation in second\n",
    "# Output: class Action represents the best action to take\n",
    "##########################################################\n",
    "def uct(board, time_limit):\n",
    "    start_time = time.time()\n",
    "    root = Node(board, None, None)\n",
    "    while (time.time() - start_time) < time_limit:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_uct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Challenge\n",
    "Time to show Stonn the power of human ingenuity! [Suggested listening](https://www.youtube.com/watch?v=_dnZHea_TI0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim.run_final_test()\n"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}